version: '3.3'

services:
  consul:
    image: consul:1.4.0
    hostname: "\{{ .Node.Hostname \}}-consul"
    volumes:
      # per-node volume to store consul persistance data
      - consul_data:/consul/data:rw
    command: agent -ui -server -bootstrap-expect {{= swarm.manager_count }}
    networks:
      - consul
      - ingress
    {{? config.expose_to_host }}
    ports:
      - target: 8500
        published: 8500
        protocol: tcp
        mode: host
    {{?}}
    configs:
      - source: consul_config_json
        target: /consul/config/server.json
    environment:
      - "SERVICE_TAGS=host-\{{ .Node.Hostname \}},instance-traefik-\{{ .Node.Hostname \}}"
    deploy:
      endpoint_mode: dnsrr
      mode: global
      update_config:
        parallelism: 1
        delay: 30s
      restart_policy:
        condition: any
      placement:
        constraints:
          - node.role == manager
      labels:
        - "traefik.docker.network=revprox_consul"
        - "traefik.port=8500"
        - "traefik.backend={{= config.consul_domain }}"
        - "traefik.frontend.rule=Host:{{= config.consul_domain }}"
        - "traefik.frontend.auth.basic.usersFile=/etc/traefik/frontend-passwd"

  # Automatically register containers as services with cosnul
  registrator:
    image: gliderlabs/registrator:master
    hostname: "\{{ .Node.Hostname \}}-registrator"
    command:
      - '-internal=true'
      - '-cleanup=true'
      - '-resync=300'
      - '-ttl=60'
      - '-ttl-refresh=45'
      - 'consul://consul:8500'
    networks:
      - consul
    volumes:
      - '/var/run/docker.sock:/tmp/docker.sock'
    deploy:
      mode: global

  # :TODO: remove this and use consul service registration instead
  #
  # We need traefik to be able to run on workers to get good scalability, however
  # traefik needs access to a docker manager socket to get service list
  # If "registrator" reliably removed old containers we could just use consul
  # as the source. Instead, we will expose the docker manager socket over tcp to
  # the worker nodes using socat
  # (see: https://blog.mikesir87.io/2018/07/letting-traefik-run-on-worker-nodes/)
  #
  # Eventually proably want to replace registrator with custom python script, doesn't
  # look too hard to implement...
  socat:
    image: alpine/socat
    command: tcp-listen:2375,fork,reuseaddr unix-connect:/var/run/docker.sock
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:rw
    networks:
      - internal
    deploy:
      mode: global
      placement:
        constraints:
          - node.role == manager

  proxy:
    image: traefik:1.7.6-alpine
    hostname: "\{{ .Node.Hostname \}}-revprox"
    ports:
      - target: 80
        published: 80
        protocol: tcp
      - target: 443
        published: 443
        protocol: tcp
      - target: 9100 # mark as exposed for registrator
        published: 9100
        protocol: tcp
    networks:
      - consul
      - ingress
      - internal
    volumes:
      - {{= stack.data_dir }}/resources/traefik.toml:/etc/traefik/traefik.toml:ro
      - {{= stack.data_dir }}/resources/frontend-passwd:/etc/traefik/frontend-passwd:ro
      # - {{= stack.data_dir }}/logs/:/var/log/traefik/:rw
    environment:
      - "DO_AUTH_TOKEN={{= secrets.do_auth_token }}"
      - "SERVICE_TAGS=host-\{{ .Node.Hostname \}},instance-traefik-\{{ .Node.Hostname \}}"
    depends_on: [ 'consul', 'traefik-init', 'socat' ]
    deploy:
      mode: global
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: any
      labels:
        - "traefik.docker.network=revprox_ingress"
        - "traefik.port=8080"
        - "traefik.backend={{= config.frontend_domain }}"
        - "traefik.frontend.rule=Host:{{= config.frontend_domain }}"
        - "traefik.frontend.auth.basic.usersFile=/etc/traefik/frontend-passwd"

configs:
  consul_config_json:
    file: files/consul_server.json

networks:
  # network that all web exposed containers should connect to
  # such that traefix proxy can forward requests
  ingress:
    driver: overlay

  # network that all applications requiring consul access should
  # connect to
  consul:
    driver: overlay

  internal:
    driver: overlay

# Persist consul_data even if container is restarted, but do so
# on a per host basis (this is local volume) since each replica has
# is own data
volumes:
  consul_data:
